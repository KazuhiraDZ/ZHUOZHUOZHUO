{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "test_valid_size = 256\n",
    "\n",
    "n_classes = 10 #MNIST total classes\n",
    "dropout = 0.75 #Dropout, probability to keep units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weights and Biases\n",
    "weights = {\n",
    "    'wc1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "    'wc2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    'wd1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "    'out':tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1':tf.Variable(tf.random_normal([32])),\n",
    "    'bc2':tf.Variable(tf.random_normal([64])),\n",
    "    'bd1':tf.Variable(tf.random_normal([1024])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,W,b,strides=1):\n",
    "    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x,k=2):\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "def conv_net(x,weights,biases,dropout):\n",
    "    #Layer 1 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'],biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1,k=2)\n",
    "    \n",
    "    #Layer 2 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1,weights['wc2'],biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2,k=2)\n",
    "    \n",
    "    #Fully connected layer 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2,[-1,weights['wd1'].get_shape().as_list()[0]]) #get_shape get properties like 3136x1024\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']),biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1,dropout)\n",
    "    \n",
    "    #Output layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1,weights['out']),biases['out'])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 97265.8281 Validation Accuray: 0.066406\n",
      "Epoch  1, Batch   2 -Loss: 78752.1875 Validation Accuray: 0.082031\n",
      "Epoch  1, Batch   3 -Loss: 65222.9570 Validation Accuray: 0.070312\n",
      "Epoch  1, Batch   4 -Loss: 46567.8047 Validation Accuray: 0.078125\n",
      "Epoch  1, Batch   5 -Loss: 38620.4062 Validation Accuray: 0.085938\n",
      "Epoch  1, Batch   6 -Loss: 32002.0039 Validation Accuray: 0.085938\n",
      "Epoch  1, Batch   7 -Loss: 27925.4961 Validation Accuray: 0.085938\n",
      "Epoch  1, Batch   8 -Loss: 28106.6172 Validation Accuray: 0.082031\n",
      "Epoch  1, Batch   9 -Loss: 26083.0605 Validation Accuray: 0.101562\n",
      "Epoch  1, Batch  10 -Loss: 23438.4219 Validation Accuray: 0.097656\n",
      "Epoch  1, Batch  11 -Loss: 23333.2402 Validation Accuray: 0.097656\n",
      "Epoch  1, Batch  12 -Loss: 21674.7422 Validation Accuray: 0.097656\n",
      "Epoch  1, Batch  13 -Loss: 22711.6895 Validation Accuray: 0.105469\n",
      "Epoch  1, Batch  14 -Loss: 24546.6289 Validation Accuray: 0.117188\n",
      "Epoch  1, Batch  15 -Loss: 23422.4727 Validation Accuray: 0.117188\n",
      "Epoch  1, Batch  16 -Loss: 23817.0703 Validation Accuray: 0.148438\n",
      "Epoch  1, Batch  17 -Loss: 17257.8066 Validation Accuray: 0.128906\n",
      "Epoch  1, Batch  18 -Loss: 19694.0703 Validation Accuray: 0.148438\n",
      "Epoch  1, Batch  19 -Loss: 18207.1953 Validation Accuray: 0.160156\n",
      "Epoch  1, Batch  20 -Loss: 18194.7656 Validation Accuray: 0.171875\n",
      "Epoch  1, Batch  21 -Loss: 17886.0078 Validation Accuray: 0.156250\n",
      "Epoch  1, Batch  22 -Loss: 17584.1660 Validation Accuray: 0.171875\n",
      "Epoch  1, Batch  23 -Loss: 14300.0527 Validation Accuray: 0.167969\n",
      "Epoch  1, Batch  24 -Loss: 15556.7949 Validation Accuray: 0.179688\n",
      "Epoch  1, Batch  25 -Loss: 13784.8359 Validation Accuray: 0.203125\n",
      "Epoch  1, Batch  26 -Loss: 15509.3496 Validation Accuray: 0.199219\n",
      "Epoch  1, Batch  27 -Loss: 14046.2500 Validation Accuray: 0.214844\n",
      "Epoch  1, Batch  28 -Loss: 15273.4629 Validation Accuray: 0.230469\n",
      "Epoch  1, Batch  29 -Loss: 13755.5137 Validation Accuray: 0.210938\n",
      "Epoch  1, Batch  30 -Loss:  9580.8203 Validation Accuray: 0.214844\n",
      "Epoch  1, Batch  31 -Loss: 14864.7754 Validation Accuray: 0.214844\n",
      "Epoch  1, Batch  32 -Loss: 12472.3125 Validation Accuray: 0.230469\n",
      "Epoch  1, Batch  33 -Loss: 12313.2949 Validation Accuray: 0.218750\n",
      "Epoch  1, Batch  34 -Loss: 10868.4199 Validation Accuray: 0.230469\n",
      "Epoch  1, Batch  35 -Loss: 12130.6055 Validation Accuray: 0.234375\n",
      "Epoch  1, Batch  36 -Loss: 10980.0020 Validation Accuray: 0.238281\n",
      "Epoch  1, Batch  37 -Loss:  9377.2686 Validation Accuray: 0.246094\n",
      "Epoch  1, Batch  38 -Loss: 10017.8438 Validation Accuray: 0.261719\n",
      "Epoch  1, Batch  39 -Loss: 10255.6787 Validation Accuray: 0.273438\n",
      "Epoch  1, Batch  40 -Loss:  8976.2852 Validation Accuray: 0.265625\n",
      "Epoch  1, Batch  41 -Loss:  9860.8623 Validation Accuray: 0.269531\n",
      "Epoch  1, Batch  42 -Loss:  9660.8281 Validation Accuray: 0.281250\n",
      "Epoch  1, Batch  43 -Loss:  9379.3770 Validation Accuray: 0.296875\n",
      "Epoch  1, Batch  44 -Loss:  9212.3945 Validation Accuray: 0.289062\n",
      "Epoch  1, Batch  45 -Loss: 11397.6582 Validation Accuray: 0.296875\n",
      "Epoch  1, Batch  46 -Loss: 12375.0400 Validation Accuray: 0.320312\n",
      "Epoch  1, Batch  47 -Loss:  8449.3779 Validation Accuray: 0.320312\n",
      "Epoch  1, Batch  48 -Loss:  8356.6611 Validation Accuray: 0.328125\n",
      "Epoch  1, Batch  49 -Loss:  9756.9883 Validation Accuray: 0.339844\n",
      "Epoch  1, Batch  50 -Loss: 10113.4238 Validation Accuray: 0.351562\n",
      "Epoch  1, Batch  51 -Loss: 11119.0273 Validation Accuray: 0.351562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b43d1f5083fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             valid_acc = sess.run(accuracy, feed_dict={x:mnist.validation.images[:test_valid_size],\n\u001b[1;32m     32\u001b[0m                                                       \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_valid_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 keep_prob:1.})\n\u001b[0m\u001b[1;32m     34\u001b[0m             print('Epoch {:>2}, Batch {:>3} -'\n\u001b[1;32m     35\u001b[0m                 'Loss: {:>10.4f} Validation Accuray: {:.6f}'.format(epoch+1,batch+1,loss,valid_acc))\n",
      "\u001b[0;32mC:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Session\n",
    "x = tf.placeholder(tf.float32,[None, 28,28,1])\n",
    "y = tf.placeholder(tf.float32,[None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#Model1\n",
    "logits = conv_net(x,weights,biases,keep_prob)\n",
    "\n",
    "#Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "#Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Launch the graph \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={x:batch_x,y:batch_y,keep_prob:dropout})\n",
    "            \n",
    "            #Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost,feed_dict={x:batch_x,y:batch_y,keep_prob:1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={x:mnist.validation.images[:test_valid_size],\n",
    "                                                      y:mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob:1.})\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                'Loss: {:>10.4f} Validation Accuray: {:.6f}'.format(epoch+1,batch+1,loss,valid_acc))\n",
    "            \n",
    "    #Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images[:test_valid_size],\n",
    "                                             y:mnist.test.images[:test_valid_size],\n",
    "                                             keep_prob:1})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3624461   2.13736652 -0.23888707 -2.43430122]\n",
      " [ 0.75313399 -0.00618586  0.6197521   1.22713884]\n",
      " [-0.15056086  1.28970347 -1.3418356   1.06598578]]\n",
      "[[ 0.3624461   2.13736652 -0.23888707 -2.43430122  0.75313399 -0.00618586\n",
      "   0.6197521   1.22713884 -0.15056086  1.28970347 -1.3418356   1.06598578]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "w_test = np.random.normal(0.0,1,(3,4))\n",
    "print(w_test)\n",
    "now = np.reshape(w_test,(-1,12))\n",
    "print(now)\n",
    "a = np.array([[1],[2],[0],[5],[3]])\n",
    "b = np.array([2,0,3,4,6])\n",
    "print(np.argmax(b,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
